{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "125/125 [==============================] - 3s 26ms/step - loss: 1.1354 - accuracy: 0.7728 - val_loss: 0.9475 - val_accuracy: 0.7579\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.7405 - accuracy: 0.8008 - val_loss: 0.7853 - val_accuracy: 0.7919\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.6388 - accuracy: 0.8307 - val_loss: 0.6954 - val_accuracy: 0.8116\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.5699 - accuracy: 0.8443 - val_loss: 0.6321 - val_accuracy: 0.8251\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.5269 - accuracy: 0.8520 - val_loss: 0.5992 - val_accuracy: 0.8290\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.5010 - accuracy: 0.8575 - val_loss: 0.5746 - val_accuracy: 0.8358\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.4798 - accuracy: 0.8622 - val_loss: 0.5524 - val_accuracy: 0.8398\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.4626 - accuracy: 0.8665 - val_loss: 0.5367 - val_accuracy: 0.8447\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.4469 - accuracy: 0.8705 - val_loss: 0.5259 - val_accuracy: 0.8469\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.4327 - accuracy: 0.8743 - val_loss: 0.5121 - val_accuracy: 0.8518\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.4203 - accuracy: 0.8780 - val_loss: 0.5015 - val_accuracy: 0.8548\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.4075 - accuracy: 0.8815 - val_loss: 0.4932 - val_accuracy: 0.8574\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.3967 - accuracy: 0.8846 - val_loss: 0.4868 - val_accuracy: 0.8603\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.3856 - accuracy: 0.8877 - val_loss: 0.4810 - val_accuracy: 0.8614\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 2s 19ms/step - loss: 0.4895 - accuracy: 0.8640 - val_loss: 0.5360 - val_accuracy: 0.8448\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 3s 20ms/step - loss: 0.4287 - accuracy: 0.8749 - val_loss: 0.5050 - val_accuracy: 0.8524\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 2s 19ms/step - loss: 0.4058 - accuracy: 0.8815 - val_loss: 0.4890 - val_accuracy: 0.8571\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 2s 19ms/step - loss: 0.3903 - accuracy: 0.8862 - val_loss: 0.4800 - val_accuracy: 0.8607\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 2s 19ms/step - loss: 0.3794 - accuracy: 0.8894 - val_loss: 0.4708 - val_accuracy: 0.8636\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 2s 19ms/step - loss: 0.3699 - accuracy: 0.8922 - val_loss: 0.4675 - val_accuracy: 0.8640\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 2s 19ms/step - loss: 0.3614 - accuracy: 0.8945 - val_loss: 0.4630 - val_accuracy: 0.8667\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.3542 - accuracy: 0.8965 - val_loss: 0.4615 - val_accuracy: 0.8672\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 2s 19ms/step - loss: 0.3466 - accuracy: 0.8985 - val_loss: 0.4549 - val_accuracy: 0.8695\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.3396 - accuracy: 0.9005 - val_loss: 0.4519 - val_accuracy: 0.8699\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.3326 - accuracy: 0.9028 - val_loss: 0.4477 - val_accuracy: 0.8714\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.3254 - accuracy: 0.9047 - val_loss: 0.4491 - val_accuracy: 0.8712\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.3184 - accuracy: 0.9069 - val_loss: 0.4444 - val_accuracy: 0.8733\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.3115 - accuracy: 0.9087 - val_loss: 0.4398 - val_accuracy: 0.8749\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.3058 - accuracy: 0.9102 - val_loss: 0.4396 - val_accuracy: 0.8754\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.2973 - accuracy: 0.9130 - val_loss: 0.4383 - val_accuracy: 0.8758\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.2913 - accuracy: 0.9142 - val_loss: 0.4368 - val_accuracy: 0.8763\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 0.2847 - accuracy: 0.9165 - val_loss: 0.4378 - val_accuracy: 0.8760\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.2780 - accuracy: 0.9185 - val_loss: 0.4335 - val_accuracy: 0.8770\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.2711 - accuracy: 0.9203 - val_loss: 0.4344 - val_accuracy: 0.8782\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.2652 - accuracy: 0.9220 - val_loss: 0.4350 - val_accuracy: 0.8775\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.2582 - accuracy: 0.9242 - val_loss: 0.4313 - val_accuracy: 0.8793\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 0.2516 - accuracy: 0.9259 - val_loss: 0.4338 - val_accuracy: 0.8799\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.2460 - accuracy: 0.9274 - val_loss: 0.4326 - val_accuracy: 0.8807\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.2398 - accuracy: 0.9295 - val_loss: 0.4356 - val_accuracy: 0.8801\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.2337 - accuracy: 0.9310 - val_loss: 0.4370 - val_accuracy: 0.8805\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.2274 - accuracy: 0.9326 - val_loss: 0.4379 - val_accuracy: 0.8809\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.2213 - accuracy: 0.9346 - val_loss: 0.4402 - val_accuracy: 0.8807\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.2155 - accuracy: 0.9362 - val_loss: 0.4415 - val_accuracy: 0.8808\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 2s 19ms/step - loss: 0.2091 - accuracy: 0.9382 - val_loss: 0.4436 - val_accuracy: 0.8805\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.2047 - accuracy: 0.9391 - val_loss: 0.4455 - val_accuracy: 0.8810\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.1983 - accuracy: 0.9413 - val_loss: 0.4454 - val_accuracy: 0.8822\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.1927 - accuracy: 0.9427 - val_loss: 0.4470 - val_accuracy: 0.8823\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.1867 - accuracy: 0.9445 - val_loss: 0.4481 - val_accuracy: 0.8821\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.1817 - accuracy: 0.9457 - val_loss: 0.4549 - val_accuracy: 0.8831\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.1777 - accuracy: 0.9471 - val_loss: 0.4550 - val_accuracy: 0.8827\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Просождите.\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Просождите.\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Просождите.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здравствейте.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здравствейте.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здравствейте.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здравствейте.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здравствейте.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Бегите!\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Бегите!\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Бегите!\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Бегите!\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Кто?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Крате!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Крате!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Крате!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Крате!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Крате!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Крате!\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Вожай!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Вожай!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: На помодь!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: На помодь!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: На помодь!\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: Прыгай!\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: Прыгай!\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Прыгай!\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Прыгай!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Остановись!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Остановись!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Остановись!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подождите!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подождите!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подождите!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подождите!\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Подождите.\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Подождите.\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Подождите.\n",
      "\n",
      "-\n",
      "Input sentence: Do it.\n",
      "Decoded sentence: Сделай это.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Продолжайте.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Продолжайте.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Привете!\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Привете!\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Привете!\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Привете!\n",
      "\n",
      "-\n",
      "Input sentence: Hurry!\n",
      "Decoded sentence: Пошибесь.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я вычела.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я вычела.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я вычела.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я вычела.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я вижу.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я вижу.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я вижу.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я подел висер.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я подел висер.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я подел висер.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я победила!\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я победила!\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я победила!\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я победила!\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Онет!\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Посныбайся.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Посныбайся.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Посныбайся.\n",
      "\n",
      "-\n",
      "Input sentence: Shoot!\n",
      "Decoded sentence: Преклани!\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбайся.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбайся.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбайся.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбайся.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбайся.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбайся.\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Соажат!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: За ваше здоровье!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: За ваше здоровье!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: За ваше здоровье!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: За ваше здоровье!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: За ваше здоровье!\n",
      "\n",
      "-\n",
      "Input sentence: Eat it.\n",
      "Decoded sentence: Лерите это.\n",
      "\n",
      "-\n",
      "Input sentence: Eat up.\n",
      "Decoded sentence: Встань.\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Замри.\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Замри.\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Замри.\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Замри.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Поднимайтесь.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Поднимайтесь.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Поднимайтесь.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди уже.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди уже.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди уже.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди уже.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди уже.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди уже.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди уже.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди уже.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Иди уже.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Понял!\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Пресливайте?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Пресливайте?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Пресливайте?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Пресливайте?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 50\n",
    "latent_dim = 256\n",
    "num_samples = 10000\n",
    "data_path = 'data/rus-eng/rus.txt'\n",
    "\n",
    "# Собираем из текстов токены и делаем pne-hot вектора на каждый токен\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поднимемся на уровень слов, чтобы можно было что-то более адекватное посчитать за адекватное время"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/rus-eng/rus.txt'\n",
    "num_samples = 10000\n",
    "\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(preprocess_sentence(input_text))\n",
    "    target_texts.append(preprocess_sentence(target_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, inp_lang_tokenizer = tokenize(input_texts)\n",
    "target_tensor, targ_lang_tokenizer = tokenize(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.lstm(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "    \n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.lstm(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.6641\n",
      "Epoch 2 Loss 0.3311\n",
      "Epoch 3 Loss 0.2262\n",
      "Epoch 4 Loss 0.1510\n",
      "Epoch 5 Loss 0.0949\n",
      "Epoch 6 Loss 0.0654\n",
      "Epoch 7 Loss 0.0504\n",
      "Epoch 8 Loss 0.0434\n",
      "Epoch 9 Loss 0.0389\n",
      "Epoch 10 Loss 0.0365\n",
      "Epoch 11 Loss 0.0338\n",
      "Epoch 12 Loss 0.0317\n",
      "Epoch 13 Loss 0.0301\n",
      "Epoch 14 Loss 0.0290\n",
      "Epoch 15 Loss 0.0269\n",
      "Epoch 16 Loss 0.0266\n",
      "Epoch 17 Loss 0.0266\n",
      "Epoch 18 Loss 0.0234\n",
      "Epoch 19 Loss 0.0233\n",
      "Epoch 20 Loss 0.0232\n",
      "Epoch 21 Loss 0.0249\n",
      "Epoch 22 Loss 0.0233\n",
      "Epoch 23 Loss 0.0212\n",
      "Epoch 24 Loss 0.0212\n",
      "Epoch 25 Loss 0.0206\n",
      "Epoch 26 Loss 0.0205\n",
      "Epoch 27 Loss 0.0195\n",
      "Epoch 28 Loss 0.0192\n",
      "Epoch 29 Loss 0.0189\n",
      "Epoch 30 Loss 0.0188\n",
      "Epoch 31 Loss 0.0186\n",
      "Epoch 32 Loss 0.0181\n",
      "Epoch 33 Loss 0.0180\n",
      "Epoch 34 Loss 0.0178\n",
      "Epoch 35 Loss 0.0170\n",
      "Epoch 36 Loss 0.0176\n",
      "Epoch 37 Loss 0.0183\n",
      "Epoch 38 Loss 0.0169\n",
      "Epoch 39 Loss 0.0163\n",
      "Epoch 40 Loss 0.0158\n",
      "Epoch 41 Loss 0.0163\n",
      "Epoch 42 Loss 0.0165\n",
      "Epoch 43 Loss 0.0159\n",
      "Epoch 44 Loss 0.0159\n",
      "Epoch 45 Loss 0.0155\n",
      "Epoch 46 Loss 0.0155\n",
      "Epoch 47 Loss 0.0161\n",
      "Epoch 48 Loss 0.0151\n",
      "Epoch 49 Loss 0.0164\n",
      "Epoch 50 Loss 0.0153\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "\n",
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 14}\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()\n",
    "    \n",
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> good morning <end>\n",
      "Predicted translation: ! <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "translate(u'good morning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "    \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "    \n",
    "    \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = len(inp_lang_tokenizer.index_word) + 2\n",
    "target_vocab_size = len(targ_lang_tokenizer.index_word) + 2\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "  \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "  \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.1659 Accuracy 0.1895\n",
      "Epoch 1 Loss 1.7365 Accuracy 0.2207\n",
      "Epoch 2 Batch 0 Loss 1.1265 Accuracy 0.2510\n",
      "Epoch 2 Loss 1.0555 Accuracy 0.2803\n",
      "Epoch 3 Batch 0 Loss 0.9498 Accuracy 0.3096\n",
      "Epoch 3 Loss 0.7396 Accuracy 0.3772\n",
      "Epoch 4 Batch 0 Loss 0.4780 Accuracy 0.4385\n",
      "Epoch 4 Loss 0.3193 Accuracy 0.4612\n",
      "Epoch 5 Batch 0 Loss 0.2378 Accuracy 0.4727\n",
      "Epoch 5 Loss 0.1852 Accuracy 0.4836\n",
      "Epoch 6 Batch 0 Loss 0.1886 Accuracy 0.4873\n",
      "Epoch 6 Loss 0.1347 Accuracy 0.4899\n",
      "Epoch 7 Batch 0 Loss 0.1711 Accuracy 0.4902\n",
      "Epoch 7 Loss 0.1227 Accuracy 0.4914\n",
      "Epoch 8 Batch 0 Loss 0.1156 Accuracy 0.4912\n",
      "Epoch 8 Loss 0.1154 Accuracy 0.4918\n",
      "Epoch 9 Batch 0 Loss 0.1017 Accuracy 0.4941\n",
      "Epoch 9 Loss 0.1077 Accuracy 0.4919\n",
      "Epoch 10 Batch 0 Loss 0.1275 Accuracy 0.4883\n",
      "Epoch 10 Loss 0.1015 Accuracy 0.4924\n",
      "Epoch 11 Batch 0 Loss 0.1349 Accuracy 0.4854\n",
      "Epoch 11 Loss 0.0947 Accuracy 0.4921\n",
      "Epoch 12 Batch 0 Loss 0.1009 Accuracy 0.4902\n",
      "Epoch 12 Loss 0.0897 Accuracy 0.4927\n",
      "Epoch 13 Batch 0 Loss 0.1006 Accuracy 0.4893\n",
      "Epoch 13 Loss 0.0810 Accuracy 0.4940\n",
      "Epoch 14 Batch 0 Loss 0.0744 Accuracy 0.4922\n",
      "Epoch 14 Loss 0.0781 Accuracy 0.4939\n",
      "Epoch 15 Batch 0 Loss 0.0658 Accuracy 0.4932\n",
      "Epoch 15 Loss 0.0678 Accuracy 0.4957\n",
      "Epoch 16 Batch 0 Loss 0.0540 Accuracy 0.4961\n",
      "Epoch 16 Loss 0.0667 Accuracy 0.4956\n",
      "Epoch 17 Batch 0 Loss 0.0437 Accuracy 0.4961\n",
      "Epoch 17 Loss 0.0640 Accuracy 0.4959\n",
      "Epoch 18 Batch 0 Loss 0.0678 Accuracy 0.4990\n",
      "Epoch 18 Loss 0.0591 Accuracy 0.4959\n",
      "Epoch 19 Batch 0 Loss 0.0297 Accuracy 0.5000\n",
      "Epoch 19 Loss 0.0579 Accuracy 0.4957\n",
      "Epoch 20 Batch 0 Loss 0.0868 Accuracy 0.4961\n",
      "Epoch 20 Loss 0.0535 Accuracy 0.4964\n",
      "Epoch 21 Batch 0 Loss 0.0434 Accuracy 0.4941\n",
      "Epoch 21 Loss 0.0512 Accuracy 0.4966\n",
      "Epoch 22 Batch 0 Loss 0.0375 Accuracy 0.4990\n",
      "Epoch 22 Loss 0.0489 Accuracy 0.4970\n",
      "Epoch 23 Batch 0 Loss 0.0419 Accuracy 0.4951\n",
      "Epoch 23 Loss 0.0524 Accuracy 0.4961\n",
      "Epoch 24 Batch 0 Loss 0.0851 Accuracy 0.4971\n",
      "Epoch 24 Loss 0.0489 Accuracy 0.4965\n",
      "Epoch 25 Batch 0 Loss 0.0447 Accuracy 0.5029\n",
      "Epoch 25 Loss 0.0445 Accuracy 0.4972\n",
      "Epoch 26 Batch 0 Loss 0.0670 Accuracy 0.4941\n",
      "Epoch 26 Loss 0.0437 Accuracy 0.4975\n",
      "Epoch 27 Batch 0 Loss 0.0323 Accuracy 0.4990\n",
      "Epoch 27 Loss 0.0416 Accuracy 0.4974\n",
      "Epoch 28 Batch 0 Loss 0.0546 Accuracy 0.4980\n",
      "Epoch 28 Loss 0.0396 Accuracy 0.4979\n",
      "Epoch 29 Batch 0 Loss 0.0260 Accuracy 0.5000\n",
      "Epoch 29 Loss 0.0402 Accuracy 0.4974\n",
      "Epoch 30 Batch 0 Loss 0.0307 Accuracy 0.4971\n",
      "Epoch 30 Loss 0.0401 Accuracy 0.4972\n",
      "Epoch 31 Batch 0 Loss 0.0326 Accuracy 0.4980\n",
      "Epoch 31 Loss 0.0402 Accuracy 0.4973\n",
      "Epoch 32 Batch 0 Loss 0.0280 Accuracy 0.4971\n",
      "Epoch 32 Loss 0.0384 Accuracy 0.4976\n",
      "Epoch 33 Batch 0 Loss 0.0238 Accuracy 0.4980\n",
      "Epoch 33 Loss 0.0365 Accuracy 0.4978\n",
      "Epoch 34 Batch 0 Loss 0.0405 Accuracy 0.4951\n",
      "Epoch 34 Loss 0.0344 Accuracy 0.4984\n",
      "Epoch 35 Batch 0 Loss 0.0566 Accuracy 0.4941\n",
      "Epoch 35 Loss 0.0341 Accuracy 0.4976\n",
      "Epoch 36 Batch 0 Loss 0.0251 Accuracy 0.5029\n",
      "Epoch 36 Loss 0.0399 Accuracy 0.4974\n",
      "Epoch 37 Batch 0 Loss 0.0315 Accuracy 0.5000\n",
      "Epoch 37 Loss 0.0351 Accuracy 0.4979\n",
      "Epoch 38 Batch 0 Loss 0.0251 Accuracy 0.5000\n",
      "Epoch 38 Loss 0.0332 Accuracy 0.4985\n",
      "Epoch 39 Batch 0 Loss 0.0357 Accuracy 0.5010\n",
      "Epoch 39 Loss 0.0322 Accuracy 0.4990\n",
      "Epoch 40 Batch 0 Loss 0.0214 Accuracy 0.5010\n",
      "Epoch 40 Loss 0.0311 Accuracy 0.4989\n",
      "Epoch 41 Batch 0 Loss 0.0302 Accuracy 0.5049\n",
      "Epoch 41 Loss 0.0303 Accuracy 0.4987\n",
      "Epoch 42 Batch 0 Loss 0.0150 Accuracy 0.5010\n",
      "Epoch 42 Loss 0.0313 Accuracy 0.4985\n",
      "Epoch 43 Batch 0 Loss 0.0339 Accuracy 0.5000\n",
      "Epoch 43 Loss 0.0305 Accuracy 0.4989\n",
      "Epoch 44 Batch 0 Loss 0.0341 Accuracy 0.4941\n",
      "Epoch 44 Loss 0.0357 Accuracy 0.4980\n",
      "Epoch 45 Batch 0 Loss 0.0135 Accuracy 0.5000\n",
      "Epoch 45 Loss 0.0300 Accuracy 0.4989\n",
      "Epoch 46 Batch 0 Loss 0.0235 Accuracy 0.5010\n",
      "Epoch 46 Loss 0.0287 Accuracy 0.4991\n",
      "Epoch 47 Batch 0 Loss 0.0172 Accuracy 0.4980\n",
      "Epoch 47 Loss 0.0281 Accuracy 0.4989\n",
      "Epoch 48 Batch 0 Loss 0.0285 Accuracy 0.4951\n",
      "Epoch 48 Loss 0.0304 Accuracy 0.4993\n",
      "Epoch 49 Batch 0 Loss 0.0253 Accuracy 0.5000\n",
      "Epoch 49 Loss 0.0278 Accuracy 0.4990\n",
      "Epoch 50 Batch 0 Loss 0.0274 Accuracy 0.5010\n",
      "Epoch 50 Loss 0.0280 Accuracy 0.4992\n",
      "Epoch 51 Batch 0 Loss 0.0201 Accuracy 0.5020\n",
      "Epoch 51 Loss 0.0258 Accuracy 0.4994\n",
      "Epoch 52 Batch 0 Loss 0.0367 Accuracy 0.4932\n",
      "Epoch 52 Loss 0.0263 Accuracy 0.4996\n",
      "Epoch 53 Batch 0 Loss 0.0271 Accuracy 0.5000\n",
      "Epoch 53 Loss 0.0306 Accuracy 0.4991\n",
      "Epoch 54 Batch 0 Loss 0.0310 Accuracy 0.4971\n",
      "Epoch 54 Loss 0.0305 Accuracy 0.4989\n",
      "Epoch 55 Batch 0 Loss 0.0244 Accuracy 0.4980\n",
      "Epoch 55 Loss 0.0291 Accuracy 0.4993\n",
      "Epoch 56 Batch 0 Loss 0.0247 Accuracy 0.5039\n",
      "Epoch 56 Loss 0.0280 Accuracy 0.4991\n",
      "Epoch 57 Batch 0 Loss 0.0238 Accuracy 0.4961\n",
      "Epoch 57 Loss 0.0257 Accuracy 0.4997\n",
      "Epoch 58 Batch 0 Loss 0.0335 Accuracy 0.5010\n",
      "Epoch 58 Loss 0.0262 Accuracy 0.4994\n",
      "Epoch 59 Batch 0 Loss 0.0211 Accuracy 0.4990\n",
      "Epoch 59 Loss 0.0257 Accuracy 0.4996\n",
      "Epoch 60 Batch 0 Loss 0.0089 Accuracy 0.5068\n",
      "Epoch 60 Loss 0.0280 Accuracy 0.4989\n",
      "Epoch 61 Batch 0 Loss 0.0231 Accuracy 0.4990\n",
      "Epoch 61 Loss 0.0250 Accuracy 0.4992\n",
      "Epoch 62 Batch 0 Loss 0.0160 Accuracy 0.5010\n",
      "Epoch 62 Loss 0.0239 Accuracy 0.4995\n",
      "Epoch 63 Batch 0 Loss 0.0252 Accuracy 0.5010\n",
      "Epoch 63 Loss 0.0283 Accuracy 0.4992\n",
      "Epoch 64 Batch 0 Loss 0.0224 Accuracy 0.5000\n",
      "Epoch 64 Loss 0.0255 Accuracy 0.4996\n",
      "Epoch 65 Batch 0 Loss 0.0173 Accuracy 0.5000\n",
      "Epoch 65 Loss 0.0262 Accuracy 0.4994\n",
      "Epoch 66 Batch 0 Loss 0.0144 Accuracy 0.5049\n",
      "Epoch 66 Loss 0.0245 Accuracy 0.4996\n",
      "Epoch 67 Batch 0 Loss 0.0182 Accuracy 0.5000\n",
      "Epoch 67 Loss 0.0255 Accuracy 0.4992\n",
      "Epoch 68 Batch 0 Loss 0.0217 Accuracy 0.5039\n",
      "Epoch 68 Loss 0.0259 Accuracy 0.4995\n",
      "Epoch 69 Batch 0 Loss 0.0155 Accuracy 0.5000\n",
      "Epoch 69 Loss 0.0242 Accuracy 0.4994\n",
      "Epoch 70 Batch 0 Loss 0.0252 Accuracy 0.5000\n",
      "Epoch 70 Loss 0.0280 Accuracy 0.4991\n",
      "Epoch 71 Batch 0 Loss 0.0325 Accuracy 0.5049\n",
      "Epoch 71 Loss 0.0233 Accuracy 0.5004\n",
      "Epoch 72 Batch 0 Loss 0.0052 Accuracy 0.5029\n",
      "Epoch 72 Loss 0.0255 Accuracy 0.4996\n",
      "Epoch 73 Batch 0 Loss 0.0190 Accuracy 0.4990\n",
      "Epoch 73 Loss 0.0273 Accuracy 0.4997\n",
      "Epoch 74 Batch 0 Loss 0.0175 Accuracy 0.5029\n",
      "Epoch 74 Loss 0.0228 Accuracy 0.4998\n",
      "Epoch 75 Batch 0 Loss 0.0236 Accuracy 0.4990\n",
      "Epoch 75 Loss 0.0244 Accuracy 0.4993\n",
      "Epoch 76 Batch 0 Loss 0.0228 Accuracy 0.4990\n",
      "Epoch 76 Loss 0.0241 Accuracy 0.4996\n",
      "Epoch 77 Batch 0 Loss 0.0178 Accuracy 0.4990\n",
      "Epoch 77 Loss 0.0230 Accuracy 0.4994\n",
      "Epoch 78 Batch 0 Loss 0.0071 Accuracy 0.5020\n",
      "Epoch 78 Loss 0.0247 Accuracy 0.5000\n",
      "Epoch 79 Batch 0 Loss 0.0257 Accuracy 0.5029\n",
      "Epoch 79 Loss 0.0250 Accuracy 0.4991\n",
      "Epoch 80 Batch 0 Loss 0.0333 Accuracy 0.4980\n",
      "Epoch 80 Loss 0.0241 Accuracy 0.4996\n",
      "Epoch 81 Batch 0 Loss 0.0142 Accuracy 0.4990\n",
      "Epoch 81 Loss 0.0244 Accuracy 0.4995\n",
      "Epoch 82 Batch 0 Loss 0.0185 Accuracy 0.4961\n",
      "Epoch 82 Loss 0.0231 Accuracy 0.4997\n",
      "Epoch 83 Batch 0 Loss 0.0197 Accuracy 0.4990\n",
      "Epoch 83 Loss 0.0243 Accuracy 0.4996\n",
      "Epoch 84 Batch 0 Loss 0.0270 Accuracy 0.4951\n",
      "Epoch 84 Loss 0.0233 Accuracy 0.4999\n",
      "Epoch 85 Batch 0 Loss 0.0176 Accuracy 0.5020\n",
      "Epoch 85 Loss 0.0266 Accuracy 0.4995\n",
      "Epoch 86 Batch 0 Loss 0.0263 Accuracy 0.5010\n",
      "Epoch 86 Loss 0.0274 Accuracy 0.4995\n",
      "Epoch 87 Batch 0 Loss 0.0103 Accuracy 0.5000\n",
      "Epoch 87 Loss 0.0288 Accuracy 0.4993\n",
      "Epoch 88 Batch 0 Loss 0.0221 Accuracy 0.4971\n",
      "Epoch 88 Loss 0.0304 Accuracy 0.4994\n",
      "Epoch 89 Batch 0 Loss 0.0451 Accuracy 0.4961\n",
      "Epoch 89 Loss 0.0249 Accuracy 0.4997\n",
      "Epoch 90 Batch 0 Loss 0.0164 Accuracy 0.5039\n",
      "Epoch 90 Loss 0.0235 Accuracy 0.5002\n",
      "Epoch 91 Batch 0 Loss 0.0290 Accuracy 0.4990\n",
      "Epoch 91 Loss 0.0216 Accuracy 0.5000\n",
      "Epoch 92 Batch 0 Loss 0.0158 Accuracy 0.4980\n",
      "Epoch 92 Loss 0.0194 Accuracy 0.5002\n",
      "Epoch 93 Batch 0 Loss 0.0189 Accuracy 0.4961\n",
      "Epoch 93 Loss 0.0226 Accuracy 0.4997\n",
      "Epoch 94 Batch 0 Loss 0.0248 Accuracy 0.4980\n",
      "Epoch 94 Loss 0.0232 Accuracy 0.4997\n",
      "Epoch 95 Batch 0 Loss 0.0266 Accuracy 0.4951\n",
      "Epoch 95 Loss 0.0255 Accuracy 0.4994\n",
      "Epoch 96 Batch 0 Loss 0.0170 Accuracy 0.5010\n",
      "Epoch 96 Loss 0.0245 Accuracy 0.5000\n",
      "Epoch 97 Batch 0 Loss 0.0325 Accuracy 0.5000\n",
      "Epoch 97 Loss 0.0250 Accuracy 0.4997\n",
      "Epoch 98 Batch 0 Loss 0.0229 Accuracy 0.5010\n",
      "Epoch 98 Loss 0.0227 Accuracy 0.4995\n",
      "Epoch 99 Batch 0 Loss 0.0299 Accuracy 0.5010\n",
      "Epoch 99 Loss 0.0233 Accuracy 0.4997\n",
      "Epoch 100 Batch 0 Loss 0.0477 Accuracy 0.5000\n",
      "Epoch 100 Loss 0.0258 Accuracy 0.4996\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        train_step(inp, tar)\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: good morning.\n",
      "Predicted translation: ['<start>', '!']\n"
     ]
    }
   ],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [1]\n",
    "    end_token = [2]\n",
    "  \n",
    "    sentence = preprocess_sentence(inp_sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    \n",
    "    encoder_input = tf.expand_dims(inputs, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "    decoder_input = [1]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(max_length_targ):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == targ_lang_tokenizer.word_index[\"<end>\"]:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "\n",
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "  \n",
    "    sentence = inp_lang_tokenizer.encode(sentence)\n",
    "  \n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "    predicted_sentence = ([targ_lang_tokenizer.index_word[i] for i in result.numpy()])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
    "        \n",
    "translate(\"good morning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: thank you.\n",
      "Predicted translation: ['<start>', '.']\n"
     ]
    }
   ],
   "source": [
    "translate(\"thank you.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
